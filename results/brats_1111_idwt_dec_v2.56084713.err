W0117 01:34:50.546452 22544090756928 torch/distributed/run.py:757] 
W0117 01:34:50.546452 22544090756928 torch/distributed/run.py:757] *****************************************
W0117 01:34:50.546452 22544090756928 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0117 01:34:50.546452 22544090756928 torch/distributed/run.py:757] *****************************************
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
  0%|          | 0/875 [00:00<?, ?it/s]  2%|▏         | 16/875 [00:00<00:05, 146.11it/s]  7%|▋         | 60/875 [00:00<00:02, 308.26it/s] 11%|█         | 98/875 [00:00<00:02, 338.67it/s]  0%|          | 0/875 [00:00<?, ?it/s] 13%|█▎        | 117/875 [00:00<00:00, 865.67it/s] 15%|█▌        | 133/875 [00:00<00:04, 168.03it/s] 19%|█▉        | 165/875 [00:00<00:03, 200.10it/s] 23%|██▎       | 204/875 [00:00<00:01, 473.52it/s] 24%|██▎       | 206/875 [00:00<00:02, 247.57it/s] 30%|██▉       | 260/875 [00:00<00:01, 487.41it/s] 30%|███       | 264/875 [00:00<00:01, 329.12it/s] 36%|███▌      | 311/875 [00:01<00:01, 364.00it/s] 36%|███▌      | 314/875 [00:00<00:01, 483.26it/s] 41%|████▏     | 363/875 [00:01<00:01, 406.19it/s] 42%|████▏     | 366/875 [00:00<00:01, 489.26it/s] 48%|████▊     | 420/875 [00:01<00:01, 450.64it/s] 48%|████▊     | 424/875 [00:00<00:00, 514.18it/s] 55%|█████▌    | 485/875 [00:01<00:00, 504.43it/s] 56%|█████▌    | 488/875 [00:00<00:00, 548.06it/s] 66%|██████▌   | 575/875 [00:01<00:00, 616.60it/s] 67%|██████▋   | 583/875 [00:01<00:00, 664.74it/s] 78%|███████▊  | 683/875 [00:01<00:00, 749.95it/s] 80%|███████▉  | 696/875 [00:01<00:00, 799.04it/s] 91%|█████████ | 798/875 [00:01<00:00, 865.83it/s] 93%|█████████▎| 817/875 [00:01<00:00, 919.83it/s]100%|██████████| 875/875 [00:01<00:00, 688.22it/s]
100%|██████████| 875/875 [00:01<00:00, 496.94it/s]
  0%|          | 0/125 [00:00<?, ?it/s]  0%|          | 0/125 [00:00<?, ?it/s] 79%|███████▉  | 99/125 [00:00<00:00, 983.42it/s] 98%|█████████▊| 122/125 [00:00<00:00, 1208.40it/s]100%|██████████| 125/125 [00:00<00:00, 1211.27it/s]
100%|██████████| 125/125 [00:00<00:00, 1010.28it/s]
  0%|          | 0/250 [00:00<?, ?it/s] 44%|████▍     | 111/250 [00:00<00:00, 1103.02it/s] 91%|█████████ | 227/250 [00:00<00:00, 1131.08it/s]100%|██████████| 250/250 [00:00<00:00, 1126.87it/s]
  0%|          | 0/250 [00:00<?, ?it/s] 44%|████▎     | 109/250 [00:00<00:00, 1084.37it/s] 90%|█████████ | 226/250 [00:00<00:00, 1130.59it/s]100%|██████████| 250/250 [00:00<00:00, 1113.79it/s]
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[rank1]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [96, 384, 1, 1, 1], strides() = [384, 1, 384, 384, 384]
bucket_view.sizes() = [96, 384, 1, 1, 1], strides() = [384, 1, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [96, 384, 1, 1, 1], strides() = [384, 1, 384, 384, 384]
bucket_view.sizes() = [96, 384, 1, 1, 1], strides() = [384, 1, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
Exception in thread Thread-11:
Traceback (most recent call last):
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
Exception in thread Thread-12:
Traceback (most recent call last):
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise e
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
Exception in thread Thread-13:
Traceback (most recent call last):
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
Exception in thread Thread-12:
Traceback (most recent call last):
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/threading.py", line 980, in _bootstrap_inner
    self.run()
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/threading.py", line 917, in run
    self._target(*self._args, **self._kwargs)
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 125, in results_loop
    raise e
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/batchgenerators/dataloading/nondet_multi_threaded_augmenter.py", line 103, in results_loop
    raise RuntimeError("One or more background workers are no longer alive. Exiting. Please check the "
RuntimeError: One or more background workers are no longer alive. Exiting. Please check the print statements above for the actual error message
srun: error: c1009a-s17: task 0: Exited with exit code 1
