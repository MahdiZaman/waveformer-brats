W0118 12:17:27.839894 22474679924544 torch/distributed/run.py:757] 
W0118 12:17:27.839894 22474679924544 torch/distributed/run.py:757] *****************************************
W0118 12:17:27.839894 22474679924544 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0118 12:17:27.839894 22474679924544 torch/distributed/run.py:757] *****************************************
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
  0%|          | 0/875 [00:00<?, ?it/s]  0%|          | 0/875 [00:00<?, ?it/s]  3%|▎         | 30/875 [00:00<00:02, 293.43it/s]  3%|▎         | 30/875 [00:00<00:02, 293.13it/s]  7%|▋         | 63/875 [00:00<00:02, 314.21it/s]  7%|▋         | 63/875 [00:00<00:02, 314.04it/s] 11%|█         | 95/875 [00:00<00:02, 311.69it/s] 11%|█         | 95/875 [00:00<00:02, 311.46it/s] 15%|█▍        | 127/875 [00:00<00:02, 305.76it/s] 15%|█▍        | 127/875 [00:00<00:02, 305.64it/s] 18%|█▊        | 159/875 [00:00<00:02, 309.87it/s] 18%|█▊        | 159/875 [00:00<00:02, 309.74it/s] 22%|██▏       | 191/875 [00:00<00:02, 312.90it/s] 22%|██▏       | 191/875 [00:00<00:02, 312.81it/s] 25%|██▌       | 223/875 [00:00<00:02, 307.84it/s] 25%|██▌       | 223/875 [00:00<00:02, 307.68it/s] 29%|██▉       | 254/875 [00:00<00:02, 300.54it/s] 29%|██▉       | 254/875 [00:00<00:02, 300.45it/s] 33%|███▎      | 285/875 [00:00<00:01, 295.60it/s] 33%|███▎      | 285/875 [00:00<00:01, 295.47it/s] 36%|███▌      | 315/875 [00:01<00:02, 268.45it/s] 36%|███▌      | 315/875 [00:01<00:02, 268.36it/s] 40%|███▉      | 346/875 [00:01<00:01, 279.23it/s] 40%|███▉      | 346/875 [00:01<00:01, 279.17it/s] 43%|████▎     | 379/875 [00:01<00:01, 291.13it/s] 43%|████▎     | 379/875 [00:01<00:01, 291.08it/s] 47%|████▋     | 412/875 [00:01<00:01, 301.48it/s] 47%|████▋     | 412/875 [00:01<00:01, 301.44it/s] 51%|█████     | 446/875 [00:01<00:01, 310.83it/s] 51%|█████     | 446/875 [00:01<00:01, 310.65it/s] 55%|█████▍    | 479/875 [00:01<00:01, 315.88it/s] 55%|█████▍    | 479/875 [00:01<00:01, 315.76it/s] 59%|█████▊    | 512/875 [00:01<00:01, 318.54it/s] 59%|█████▊    | 512/875 [00:01<00:01, 318.39it/s] 62%|██████▏   | 544/875 [00:01<00:01, 318.10it/s] 62%|██████▏   | 544/875 [00:01<00:01, 318.03it/s] 66%|██████▌   | 578/875 [00:01<00:00, 321.62it/s] 66%|██████▌   | 578/875 [00:01<00:00, 321.46it/s] 70%|██████▉   | 612/875 [00:01<00:00, 325.42it/s] 70%|██████▉   | 612/875 [00:01<00:00, 325.32it/s] 74%|███████▍  | 646/875 [00:02<00:00, 328.80it/s] 74%|███████▍  | 646/875 [00:02<00:00, 328.66it/s] 78%|███████▊  | 679/875 [00:02<00:00, 320.75it/s] 78%|███████▊  | 679/875 [00:02<00:00, 320.64it/s] 81%|████████▏ | 712/875 [00:02<00:00, 320.14it/s] 81%|████████▏ | 712/875 [00:02<00:00, 320.00it/s] 85%|████████▌ | 745/875 [00:02<00:00, 316.97it/s] 85%|████████▌ | 745/875 [00:02<00:00, 316.87it/s] 89%|████████▉ | 779/875 [00:02<00:00, 321.80it/s] 89%|████████▉ | 779/875 [00:02<00:00, 321.66it/s] 93%|█████████▎| 812/875 [00:02<00:00, 320.38it/s] 93%|█████████▎| 812/875 [00:02<00:00, 320.26it/s] 97%|█████████▋| 845/875 [00:02<00:00, 313.03it/s] 97%|█████████▋| 845/875 [00:02<00:00, 312.91it/s]100%|██████████| 875/875 [00:02<00:00, 310.55it/s]
100%|██████████| 875/875 [00:02<00:00, 310.54it/s]
  0%|          | 0/125 [00:00<?, ?it/s]  0%|          | 0/125 [00:00<?, ?it/s] 25%|██▍       | 31/125 [00:00<00:00, 309.97it/s] 34%|███▎      | 42/125 [00:00<00:00, 413.49it/s] 50%|████▉     | 62/125 [00:00<00:00, 270.58it/s] 67%|██████▋   | 84/125 [00:00<00:00, 307.33it/s] 75%|███████▌  | 94/125 [00:00<00:00, 288.42it/s] 94%|█████████▎| 117/125 [00:00<00:00, 309.15it/s]100%|██████████| 125/125 [00:00<00:00, 319.69it/s]
100%|██████████| 125/125 [00:00<00:00, 296.12it/s]
  0%|          | 0/250 [00:00<?, ?it/s] 10%|█         | 26/250 [00:00<00:00, 255.81it/s] 21%|██        | 53/250 [00:00<00:00, 263.93it/s]  0%|          | 0/250 [00:00<?, ?it/s] 32%|███▏      | 80/250 [00:00<00:00, 257.60it/s] 38%|███▊      | 96/250 [00:00<00:00, 942.62it/s] 43%|████▎     | 108/250 [00:00<00:00, 263.99it/s] 56%|█████▌    | 140/250 [00:00<00:00, 281.55it/s] 68%|██████▊   | 169/250 [00:00<00:00, 281.50it/s] 76%|███████▋  | 191/250 [00:00<00:00, 426.62it/s] 81%|████████  | 203/250 [00:00<00:00, 298.76it/s] 94%|█████████▍| 235/250 [00:00<00:00, 303.64it/s] 99%|█████████▉| 248/250 [00:00<00:00, 382.53it/s]100%|██████████| 250/250 [00:00<00:00, 418.70it/s]
100%|██████████| 250/250 [00:00<00:00, 287.62it/s]
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
[rank0]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W reducer.cpp:1389] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [96, 384, 1, 1, 1], strides() = [384, 1, 384, 384, 384]
bucket_view.sizes() = [96, 384, 1, 1, 1], strides() = [384, 1, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/autograd/graph.py:744: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [96, 384, 1, 1, 1], strides() = [384, 1, 384, 384, 384]
bucket_view.sizes() = [96, 384, 1, 1, 1], strides() = [384, 1, 1, 1, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank1]:[E ProcessGroupNCCL.cpp:563] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=317851, OpType=BROADCAST, NumelIn=2097152, NumelOut=2097152, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
[rank1]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 1] Timeout at NCCL work: 317851, last enqueued NCCL work: 317851, last completed NCCL work: 317850.
[rank1]:[E ProcessGroupNCCL.cpp:577] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.
[rank1]:[E ProcessGroupNCCL.cpp:583] [Rank 1] To avoid data inconsistency, we are taking the entire process down.
[rank1]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=317851, OpType=BROADCAST, NumelIn=2097152, NumelOut=2097152, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x146da600e897 in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x146da72e7c62 in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x146da72eca80 in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x146da72eddcc in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x146df2d90b65 in /blue/r.forghani/envs/waveformer/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x146df82871ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x146df77588d3 in /lib64/libc.so.6)

terminate called after throwing an instance of 'c10::DistBackendError'
  what():  [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=317851, OpType=BROADCAST, NumelIn=2097152, NumelOut=2097152, Timeout(ms)=600000) ran for 600042 milliseconds before timing out.
Exception raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x146da600e897 in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x146da72e7c62 in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x146da72eca80 in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x146da72eddcc in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #4: <unknown function> + 0xd3b65 (0x146df2d90b65 in /blue/r.forghani/envs/waveformer/bin/../lib/libstdc++.so.6)
frame #5: <unknown function> + 0x81ca (0x146df82871ca in /lib64/libpthread.so.0)
frame #6: clone + 0x43 (0x146df77588d3 in /lib64/libc.so.6)

Exception raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):
frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x146da600e897 in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0xe32119 (0x146da6f71119 in /blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/lib/libtorch_cuda.so)
frame #2: <unknown function> + 0xd3b65 (0x146df2d90b65 in /blue/r.forghani/envs/waveformer/bin/../lib/libstdc++.so.6)
frame #3: <unknown function> + 0x81ca (0x146df82871ca in /lib64/libpthread.so.0)
frame #4: clone + 0x43 (0x146df77588d3 in /lib64/libc.so.6)

W0119 11:36:54.061478 22474679924544 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2396510 closing signal SIGTERM
E0119 11:36:55.005378 22474679924544 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: -6) local_rank: 1 (pid: 2396511) of binary: /blue/r.forghani/envs/waveformer/bin/python3.9
Traceback (most recent call last):
  File "/blue/r.forghani/envs/waveformer/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/blue/r.forghani/envs/waveformer/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/blue/r.forghani/mdmahfuzalhasan/scripts/SegMamba/3_train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-01-19_11:36:54
  host      : c0910a-s29.ufhpc
  rank      : 1 (local_rank: 1)
  exitcode  : -6 (pid: 2396511)
  error_file: <N/A>
  traceback : Signal 6 (SIGABRT) received by PID 2396511
============================================================
srun: error: c0910a-s29: task 0: Exited with exit code 1
